{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#-*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import time\n",
    "import csv\n",
    "\n",
    "#check start time to calculate running time\n",
    "start_time = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n\\nfrom bs4 import BeautifulSoup\\nfrom selenium import webdriver\\n# import datetime\\n# import time\\n# import csv\\n\\n\\n# check start time to calculate running time\\nstart_time = time.time()\\n\\n# initialize url addresses to crawl\\nhome_url = \"https://www.instagram.com/\"\\nopen_url = home_url + \" \" # attatch the remaining address to open\\ntarget_url = \"\" # target page url\\nimage = \"\" # image source url\\ncount = 0\\n\\n# Build web browser with selenium.\\n# Assign your path where the webdriver file located to \\'executable_path\\' as String\\ndriver = webdriver.Firefox(executable_path=\\'/usr/local/bin/geckodriver\\')\\ndriver.get(open_url)\\n\\n# Build bs4\\nsoup = BeautifulSoup(driver.page_source, \\'lxml\\')\\n\\n# see more\\nelement = driver.find_element_by_class_name(\\'_8imhp\\')\\nelement.click()\\nitems_len = 0\\nwhile(1):\\n    for i in range(3):\\n        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\\n        time.sleep(1.5) # waiting interval\\n    \\n    # Build bs4\\n    soup = BeautifulSoup(driver.page_source, \\'lxml\\')\\n    items = soup.find_all(\"div\", class_=\"_8mlbc _vbtk2 _t5r8b\")\\n    if items_len != len(items):\\n        items_len = len(items)\\n        print(\"number of items: \" + str(items_len))\\n    elif items_len == len(items):\\n        print(\"end of page\")\\n        break\\n    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\\n    \\nf = open(\\'/Users/choiyoungrok/OneDrive/Data_Science/dataset/instagram_downtwnr.csv\\', \\'w\\')\\nwr = csv.writer(f)\\nwr.writerow([\\'uid\\', \\'datetime\\', \\'likes\\', \\'location\\', \\'account\\', \\'tags\\', \\'comments\\', \\'image\\'])\\nf.close()    \\n\\nfor x in items:\\n    count += 1\\n    uid = \"\"\\n    tags = \"\"\\n    datetime = None\\n    likes = None\\n    location = None\\n    account = None\\n    comments = \"\"\\n    \\n    # URL\\n    target_url = x.get(\\'href\\').split(\\'?\\')[0]\\n    uid = target_url.split(\"/\")[2]\\n    target_url = open_url + target_url\\n    driver.get(target_url)\\n    \\n    # Image\\n    image = x.find(\"div\", class_=\"_22yr2\").find(\"div\", class_=\"_jjzlb\").find(\"img\").get(\"src\")\\n    \\n    # Open Target Page\\n    soup2 = BeautifulSoup(driver.page_source, \\'lxml\\')\\n    # Time\\n    if soup2.find(\"time\") != None:\\n        datetime = soup2.find(\"time\").get(\\'datetime\\')\\n    \\n    # Location\\n    location_container = soup2.find(\"a\", class_=\"_kul9p _mg7fs\")\\n    if location_container != None:\\n        location = location_container.get(\\'href\\').split(\\'/\\')[3]+\\'$\\'+location_container.get(\\'title\\').encode(\\'utf-8\\')\\n    \\n    # Account\\n    if soup2.find(\"a\", class_=\"_4zhc5 notranslate _jozwt\") != None:\\n        account = soup2.find(\"a\", class_=\"_4zhc5 notranslate _jozwt\").get(\\'title\\')\\n    \\n    # Tag\\n    tags_container = soup2.find_all(\"a\")\\n    for tag in tags_container:\\n        if (tag.get(\\'href\\') != None) and (\\'#\\' in tag.getText()):\\n                tags += tag.getText()\\n    \\n    if soup2.find(\"div\", class_=\"_kkf84 _oajsw\") == None: # if nobody likes yet\\n        likes = 0\\n    else: # caculating small number of likes\\n        like_container = soup2.find(\"div\", class_=\"_kkf84 _oajsw\").find(\"span\", class_=\"_tf9x3\")\\n        if like_container == None:\\n            likes = len(soup2.find(\"div\", class_=\"_kkf84 _oajsw\").find_all(\"a\"))\\n        elif like_container.find(\"span\") != None:\\n            likes = like_container.find(\"span\").getText()\\n        else: # https://www.instagram.com/p/6ZON55AFfa/\\n            likes = like_container.getText().encode(\\'utf-8\\')[-4:-3]\\n\\n    \\n    # comments\\n    if soup2.find(\"button\", class_=\"_jpmen _8zx0v\") != None: # check if \\'\\xeb\\x8c\\x93\\xea\\xb8\\x80 \\xeb\\x8d\\x94 \\xeb\\xb3\\xb4\\xea\\xb8\\xb0 exists\\n        element = driver.find_element_by_class_name(\\'_jpmen\\')\\n        element.click() \\n        \\n    comments_container = soup2.find_all(\"li\", class_=\"_99ch8\")\\n    if comments_container != None:\\n        for comment in comments_container:\\n            if comment.find(\"a\", class_=\"_4zhc5 notranslate _ebg8h\") != None:\\n                comments = comments + \"<^\" + comment.find(\"a\", class_=\"_4zhc5 notranslate _ebg8h\").get(\\'title\\') + \">$\"\\n                # indentifier(<^: account, >$: text)\\n                comments = comments + comment.span.getText()\\n    \\n    print(\"===={}====\").format(count)\\n    print(\"uid: {}\").format(uid)\\n    \\n    #print(\"url: {}\").format(target_url)\\n    print(\"datetime: {}\").format(datetime)\\n    #print(\"likes: {}\").format(likes)\\n    #pprint(location)\\n    #print(\"image: {}\").format(image)\\n    #print(\"account: {}\").format(account)\\n    #print(tags)\\n    #print(comments)\\n    #print(\" \")\\n    \\n    f = open(\\'/Users/choiyoungrok/OneDrive/Data_Science/dataset/instagram_downtwnr.csv\\', \\'a\\')\\n    wr = csv.writer(f)\\n    wr.writerow([uid, datetime, likes, location, account, tags.encode(\\'utf-8\\'), comments.encode(\\'utf-8\\')])\\n    f.close()\\n\\nend_time = time.time()\\nrunning_time = end_time - start_time\\nprint(\"Total running time: {} minutes\").format(running_time / 60)\\n'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "#-*- coding: utf-8 -*-\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "# import datetime\n",
    "# import time\n",
    "# import csv\n",
    "\n",
    "\n",
    "# check start time to calculate running time\n",
    "start_time = time.time()\n",
    "\n",
    "# initialize url addresses to crawl\n",
    "home_url = \"https://www.instagram.com/\"\n",
    "open_url = home_url + \" \" # attatch the remaining address to open\n",
    "target_url = \"\" # target page url\n",
    "image = \"\" # image source url\n",
    "count = 0\n",
    "\n",
    "# Build web browser with selenium.\n",
    "# Assign your path where the webdriver file located to 'executable_path' as String\n",
    "driver = webdriver.Firefox(executable_path='/usr/local/bin/geckodriver')\n",
    "driver.get(open_url)\n",
    "\n",
    "# Build bs4\n",
    "soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "\n",
    "# see more\n",
    "element = driver.find_element_by_class_name('_8imhp')\n",
    "element.click()\n",
    "items_len = 0\n",
    "while(1):\n",
    "    for i in range(3):\n",
    "        driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "        time.sleep(1.5) # waiting interval\n",
    "    \n",
    "    # Build bs4\n",
    "    soup = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    items = soup.find_all(\"div\", class_=\"_8mlbc _vbtk2 _t5r8b\")\n",
    "    if items_len != len(items):\n",
    "        items_len = len(items)\n",
    "        print(\"number of items: \" + str(items_len))\n",
    "    elif items_len == len(items):\n",
    "        print(\"end of page\")\n",
    "        break\n",
    "    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "    \n",
    "f = open('/Users/choiyoungrok/OneDrive/Data_Science/dataset/instagram_downtwnr.csv', 'w')\n",
    "wr = csv.writer(f)\n",
    "wr.writerow(['uid', 'datetime', 'likes', 'location', 'account', 'tags', 'comments', 'image'])\n",
    "f.close()    \n",
    "\n",
    "for x in items:\n",
    "    count += 1\n",
    "    uid = \"\"\n",
    "    tags = \"\"\n",
    "    datetime = None\n",
    "    likes = None\n",
    "    location = None\n",
    "    account = None\n",
    "    comments = \"\"\n",
    "    \n",
    "    # URL\n",
    "    target_url = x.get('href').split('?')[0]\n",
    "    uid = target_url.split(\"/\")[2]\n",
    "    target_url = open_url + target_url\n",
    "    driver.get(target_url)\n",
    "    \n",
    "    # Image\n",
    "    image = x.find(\"div\", class_=\"_22yr2\").find(\"div\", class_=\"_jjzlb\").find(\"img\").get(\"src\")\n",
    "    \n",
    "    # Open Target Page\n",
    "    soup2 = BeautifulSoup(driver.page_source, 'lxml')\n",
    "    # Time\n",
    "    if soup2.find(\"time\") != None:\n",
    "        datetime = soup2.find(\"time\").get('datetime')\n",
    "    \n",
    "    # Location\n",
    "    location_container = soup2.find(\"a\", class_=\"_kul9p _mg7fs\")\n",
    "    if location_container != None:\n",
    "        location = location_container.get('href').split('/')[3]+'$'+location_container.get('title').encode('utf-8')\n",
    "    \n",
    "    # Account\n",
    "    if soup2.find(\"a\", class_=\"_4zhc5 notranslate _jozwt\") != None:\n",
    "        account = soup2.find(\"a\", class_=\"_4zhc5 notranslate _jozwt\").get('title')\n",
    "    \n",
    "    # Tag\n",
    "    tags_container = soup2.find_all(\"a\")\n",
    "    for tag in tags_container:\n",
    "        if (tag.get('href') != None) and ('#' in tag.getText()):\n",
    "                tags += tag.getText()\n",
    "    \n",
    "    if soup2.find(\"div\", class_=\"_kkf84 _oajsw\") == None: # if nobody likes yet\n",
    "        likes = 0\n",
    "    else: # caculating small number of likes\n",
    "        like_container = soup2.find(\"div\", class_=\"_kkf84 _oajsw\").find(\"span\", class_=\"_tf9x3\")\n",
    "        if like_container == None:\n",
    "            likes = len(soup2.find(\"div\", class_=\"_kkf84 _oajsw\").find_all(\"a\"))\n",
    "        elif like_container.find(\"span\") != None:\n",
    "            likes = like_container.find(\"span\").getText()\n",
    "        else: # https://www.instagram.com/p/6ZON55AFfa/\n",
    "            likes = like_container.getText().encode('utf-8')[-4:-3]\n",
    "\n",
    "    \n",
    "    # comments\n",
    "    if soup2.find(\"button\", class_=\"_jpmen _8zx0v\") != None: # check if '댓글 더 보기 exists\n",
    "        element = driver.find_element_by_class_name('_jpmen')\n",
    "        element.click() \n",
    "        \n",
    "    comments_container = soup2.find_all(\"li\", class_=\"_99ch8\")\n",
    "    if comments_container != None:\n",
    "        for comment in comments_container:\n",
    "            if comment.find(\"a\", class_=\"_4zhc5 notranslate _ebg8h\") != None:\n",
    "                comments = comments + \"<^\" + comment.find(\"a\", class_=\"_4zhc5 notranslate _ebg8h\").get('title') + \">$\"\n",
    "                # indentifier(<^: account, >$: text)\n",
    "                comments = comments + comment.span.getText()\n",
    "    \n",
    "    print(\"===={}====\").format(count)\n",
    "    print(\"uid: {}\").format(uid)\n",
    "    \n",
    "    #print(\"url: {}\").format(target_url)\n",
    "    print(\"datetime: {}\").format(datetime)\n",
    "    #print(\"likes: {}\").format(likes)\n",
    "    #pprint(location)\n",
    "    #print(\"image: {}\").format(image)\n",
    "    #print(\"account: {}\").format(account)\n",
    "    #print(tags)\n",
    "    #print(comments)\n",
    "    #print(\" \")\n",
    "    \n",
    "    f = open('/Users/choiyoungrok/OneDrive/Data_Science/dataset/instagram_downtwnr.csv', 'a')\n",
    "    wr = csv.writer(f)\n",
    "    wr.writerow([uid, datetime, likes, location, account, tags.encode('utf-8'), comments.encode('utf-8')])\n",
    "    f.close()\n",
    "\n",
    "end_time = time.time()\n",
    "running_time = end_time - start_time\n",
    "print(\"Total running time: {} minutes\").format(running_time / 60)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
